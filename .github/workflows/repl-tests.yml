name: Comprehensive Test Suite

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'test/**'
      - 'scripts/**'
      - 'benchmarks/**'
      - 'lsp/**'
      - 'jest.config*.js'
      - 'package.json'
      - 'tsconfig*.json'
      - '.github/workflows/repl-tests.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'test/**'
      - 'scripts/**'
      - 'benchmarks/**'
      - 'lsp/**'
      - 'jest.config*.js'
      - 'package.json'
      - 'tsconfig*.json'
      - '.github/workflows/repl-tests.yml'

jobs:
  # Run core Jest tests once with fast SWC compilation
  core-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Run Jest tests (SWC)
      run: npm run test:fast
      
    - name: Cache Jest transform cache
      uses: actions/cache@v4
      with:
        path: .jest-cache
        key: jest-cache-${{ runner.os }}-${{ hashFiles('**/package-lock.json') }}-${{ hashFiles('src/**/*.ts', 'test/**/*.ts') }}
        restore-keys: |
          jest-cache-${{ runner.os }}-${{ hashFiles('**/package-lock.json') }}-
          jest-cache-${{ runner.os }}-

  # Run REPL-specific tests separately
  repl-tests:
    runs-on: ubuntu-latest
    needs: core-tests  # Only run if core tests pass
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Run simple REPL tests
      run: npm run test:repl-simple
    
    - name: Run REPL automation tests
      run: npm run test:repl-automation
      
    - name: Upload REPL test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: repl-test-artifacts
        path: |
          repl-test-*.log
          test-automation-*.json
          test-results/
        retention-days: 14

  # Run LSP tests separately
  lsp-tests:
    runs-on: ubuntu-latest
    needs: core-tests  # Only run if core tests pass
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
    
    - name: Setup Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        override: true
    
    - name: Cache Rust dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          lsp/target
        key: rust-${{ runner.os }}-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          rust-${{ runner.os }}-
    
    - name: Install dependencies
      run: npm ci
    
    - name: Build TypeScript project
      run: npm run build
    
    - name: Build LSP server
      run: |
        cd lsp
        cargo build --release
    
    - name: Run LSP unit tests
      run: |
        cd lsp
        cargo test
    
    - name: Test LSP integration
      run: |
        cd lsp
        ./test_enhanced_lsp.sh
    
    - name: Upload LSP test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: lsp-test-artifacts
        path: |
          lsp/target/release/noolang-lsp
          lsp/test-*.log
        retention-days: 14

  # Run benchmarks (can run in parallel with other tests)
  benchmarks:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Run core benchmarks
      run: |
        npm run benchmark
        # Validate that benchmark results were generated
        if [ ! -d "benchmark-results" ] || [ -z "$(find benchmark-results -name '*.json' 2>/dev/null)" ]; then
          echo "::error::Core benchmarks failed to generate results"
          exit 1
        fi
        echo "âœ… Core benchmarks completed successfully"
      
    - name: Run REPL benchmarks (allowed to fail)
      run: |
        set +e  # Don't exit on error for REPL benchmarks
        npm run benchmark:repl
        REPL_EXIT_CODE=$?
        
        if [ $REPL_EXIT_CODE -eq 0 ]; then
          echo "âœ… REPL benchmarks completed successfully"
          echo "REPL_STATUS=success" >> $GITHUB_ENV
        else
          echo "âš ï¸ REPL benchmarks failed - this is not blocking"
          echo "::warning::REPL benchmarks failed but pipeline continues"
          echo "REPL_STATUS=failed" >> $GITHUB_ENV
        fi
        
        set -e  # Re-enable exit on error
    
    - name: Validate benchmark results
      run: |
        # Check core benchmark results exist and are valid
        CORE_RESULTS=$(find benchmark-results -name "results-*.json" 2>/dev/null)
        if [ -z "$CORE_RESULTS" ]; then
          echo "::error::No core benchmark results found"
          exit 1
        fi
        
        # Validate JSON structure
        for file in $CORE_RESULTS; do
          if ! jq empty "$file" 2>/dev/null; then
            echo "::error::Invalid JSON in benchmark file: $file"
            exit 1
          fi
          
          # Check for required fields
          if ! jq -e '.results | length > 0' "$file" >/dev/null; then
            echo "::error::No benchmark results in file: $file"
            exit 1
          fi
        done
        
        echo "âœ… Benchmark validation passed"
        
        # Add results summary to job
        echo "## Benchmark Results" >> $GITHUB_STEP_SUMMARY
        echo "| Benchmark | Phase | Min | Max | Avg | Median |" >> $GITHUB_STEP_SUMMARY
        echo "|-----------|-------|-----|-----|-----|--------|" >> $GITHUB_STEP_SUMMARY
        
        # Extract and display results
        for file in $CORE_RESULTS; do
          jq -r '.results[] | "| \(.benchmark) | \(.phase) | \(.min)ms | \(.max)ms | \(.avg)ms | \(.median)ms |"' "$file" >> $GITHUB_STEP_SUMMARY
        done
        
        if [ "$REPL_STATUS" = "failed" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âš ï¸ **Note**: REPL benchmarks failed and were skipped" >> $GITHUB_STEP_SUMMARY
        fi
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: |
          benchmark-results/
        retention-days: 90

  # Performance comparison for PRs
  performance-comparison:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
      
    - name: Cache Jest transform cache
      uses: actions/cache@v4
      with:
        path: .jest-cache
        key: jest-cache-${{ runner.os }}-${{ hashFiles('**/package-lock.json') }}-${{ hashFiles('src/**/*.ts', 'test/**/*.ts') }}
        restore-keys: |
          jest-cache-${{ runner.os }}-${{ hashFiles('**/package-lock.json') }}-
          jest-cache-${{ runner.os }}-
      
    - name: Benchmark current branch
      run: |
        echo "Benchmarking current branch..."
        npm run benchmark:all || echo "Current branch benchmarks failed"
        if [ -d "benchmark-results" ]; then
          mv benchmark-results benchmark-results-current
        else
          echo "No benchmark results from current branch"
        fi
    
    - name: Checkout main branch
      run: |
        git reset --hard HEAD
        git clean -fd
        git checkout origin/main
      
    - name: Install dependencies and build (main)
      run: |
        npm ci
        npm run build
      
    - name: Benchmark main branch
      run: |
        echo "Benchmarking main branch..."
        npm run benchmark:all || echo "Main branch benchmarks failed"
        if [ -d "benchmark-results" ]; then
          mv benchmark-results benchmark-results-main
        else
          echo "No benchmark results from main branch"
        fi
    
    - name: Compare performance
      run: |
        echo "## Performance Comparison" >> $GITHUB_STEP_SUMMARY
        echo "Comparing current branch against main..." >> $GITHUB_STEP_SUMMARY
        
        # Check if benchmark files exist with proper globbing
        CURRENT_RESULTS=$(find benchmark-results-current -name "*.json" 2>/dev/null | head -1)
        MAIN_RESULTS=$(find benchmark-results-main -name "*.json" 2>/dev/null | head -1)
        
        if [ -n "$CURRENT_RESULTS" ] && [ -n "$MAIN_RESULTS" ]; then
          echo "âœ… Benchmark files found" >> $GITHUB_STEP_SUMMARY
          
          # Add basic performance validation
          echo "### Performance Analysis" >> $GITHUB_STEP_SUMMARY
          
          # Extract performance metrics and compare
          CURRENT_AVG=$(grep -o '"avg":"[0-9.]*"' "$CURRENT_RESULTS" | head -1 | cut -d'"' -f4)
          MAIN_AVG=$(grep -o '"avg":"[0-9.]*"' "$MAIN_RESULTS" | head -1 | cut -d'"' -f4)
          
          if [ -n "$CURRENT_AVG" ] && [ -n "$MAIN_AVG" ]; then
            echo "- Current branch average: ${CURRENT_AVG}ms" >> $GITHUB_STEP_SUMMARY
            echo "- Main branch average: ${MAIN_AVG}ms" >> $GITHUB_STEP_SUMMARY
            
            # Calculate percentage difference (using awk for floating point)
            DIFF_PERCENT=$(awk "BEGIN {printf \"%.1f\", (($CURRENT_AVG - $MAIN_AVG) / $MAIN_AVG) * 100}")
            echo "- Performance difference: ${DIFF_PERCENT}%" >> $GITHUB_STEP_SUMMARY
            
            # Flag significant regressions
            if awk "BEGIN {exit ($DIFF_PERCENT > 20)}"; then
              echo "âš ï¸ **WARNING: Performance regression detected (>20% slower)**" >> $GITHUB_STEP_SUMMARY
              echo "::warning::Performance regression: ${DIFF_PERCENT}% slower than main"
            elif awk "BEGIN {exit ($DIFF_PERCENT < -10)}"; then
              echo "ðŸš€ **Performance improvement detected (>10% faster)**" >> $GITHUB_STEP_SUMMARY
            else
              echo "âœ… Performance within acceptable range" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "âš ï¸ Could not extract performance metrics for comparison" >> $GITHUB_STEP_SUMMARY
          fi
        else
          echo "âš ï¸ Could not generate performance comparison - benchmark files missing" >> $GITHUB_STEP_SUMMARY
          echo "::warning::Benchmark comparison failed - results files not found"
        fi
    
    - name: Upload comparison results
      uses: actions/upload-artifact@v4
      with:
        name: performance-comparison
        path: |
          benchmark-results-current/
          benchmark-results-main/
        retention-days: 30