name: Comprehensive Test Suite

on:
  push:
    branches: [ main, develop, 'cursor/**' ]  # Added cursor/** pattern to include our branch
    paths:
      - 'src/**'
      - 'test/**'
      - 'scripts/**'
      - 'benchmarks/**'
      - 'lsp/**'
      - 'examples/**'
      - 'docs/**'
      - 'README.md'
      - 'jest.config*.js'
      - 'package.json'
      - 'tsconfig*.json'
      - '.github/workflows/ci.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'test/**'
      - 'scripts/**'
      - 'benchmarks/**'
      - 'lsp/**'
      - 'examples/**'
      - 'docs/**'
      - 'README.md'
      - 'jest.config*.js'
      - 'package.json'
      - 'tsconfig*.json'
      - '.github/workflows/ci.yml'
  workflow_dispatch:  # Allow manual triggering

jobs:
  # TypeScript type checking for both source and test files
  # This ensures all TypeScript code (including tests) compiles without errors
  # Source files use main tsconfig.json, test files use tsconfig.test.json
  typescript-check:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: TypeScript check - source files
      run: npm run typecheck
      
    - name: TypeScript check - test files (including assert helpers)
      run: npm run typecheck:test
      
    - name: Report TypeScript status
      run: echo "✅ TypeScript checking passed for source and test files"

  # Run core tests with both Jest and uvu
  core-tests:
    runs-on: ubuntu-latest
    needs: typescript-check  # Only run tests if TypeScript checking passes
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Run Jest tests (SWC)
      run: npm run test:fast
    
    - name: Run uvu tests
      run: npm run test:uvu
      
    - name: Cache Jest transform cache
      uses: actions/cache@v4
      with:
        path: .jest-cache
        key: jest-cache-${{ runner.os }}-${{ hashFiles('**/package-lock.json') }}-${{ hashFiles('src/**/*.ts', 'test/**/*.ts') }}
        restore-keys: |
          jest-cache-${{ runner.os }}-${{ hashFiles('**/package-lock.json') }}-
          jest-cache-${{ runner.os }}-

  # Run REPL-specific tests separately  
  repl-tests:
    runs-on: ubuntu-latest
    needs: [typescript-check, core-tests]  # Only run if TypeScript check and core tests pass
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Run simple REPL tests
      run: npm run test:repl-simple
    
    - name: Run REPL automation tests
      run: npm run test:repl-automation
      
    - name: Upload REPL test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: repl-test-artifacts
        path: |
          repl-test-*.log
          test-automation-*.json
          test-results/
        retention-days: 14

  # Run LSP tests separately
  lsp-tests:
    runs-on: ubuntu-latest
    needs: [typescript-check, core-tests]  # Only run if TypeScript check and core tests pass
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
    
    - name: Setup Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        override: true
    
    - name: Cache Rust dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          lsp/target
        key: rust-${{ runner.os }}-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          rust-${{ runner.os }}-
    
    - name: Install dependencies
      run: npm ci
    
    - name: Build TypeScript project
      run: npm run build
    
    - name: Build LSP server
      run: |
        cd lsp
        cargo build --release
    
    - name: Run LSP core tests
      run: |
        cd lsp
        chmod +x run_core_tests.sh
        ./run_core_tests.sh
    
    - name: Run LSP comprehensive tests
      run: |
        cd lsp
        cargo test
    
    - name: Test LSP integration
      run: |
        cd lsp
        ./test_enhanced_lsp.sh
    
    - name: Upload LSP test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: lsp-test-artifacts
        path: |
          lsp/target/release/noolang-lsp
          lsp/test-*.log
        retention-days: 14

  # Validate example files (currently optional due to known type system issues)
  example-validation:
    runs-on: ubuntu-latest
    needs: [typescript-check, core-tests]  # Only run if TypeScript check and core tests pass
    continue-on-error: true  # Don't fail the entire pipeline if examples fail
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Build project
      run: npm run build
    
    - name: Test working examples
      run: |
        echo "Testing known working examples..."
        
        # List of examples that should work
        WORKING_EXAMPLES=(
          "basic.noo"
          "adt_demo.noo" 
          "safe_thrush_demo.noo"
          "simple_adt.noo"
          "minimal_trait_test.noo"
          "math_functions.noo"
        )
        
        FAILED_EXAMPLES=()
        
        for example in "${WORKING_EXAMPLES[@]}"; do
          echo "Testing examples/$example..."
          if npm start "examples/$example"; then
            echo "✅ $example passed"
          else
            echo "❌ $example failed"
            FAILED_EXAMPLES+=("$example")
          fi
        done
        
                 if [ ${#FAILED_EXAMPLES[@]} -eq 0 ]; then
           echo "✅ All working examples passed validation"
         else
           echo "⚠️ Failed examples: ${FAILED_EXAMPLES[*]}"
           echo "::warning::Some examples that should work are failing - this indicates type system regressions"
           # Don't exit 1 here since continue-on-error is set
         fi
    
    - name: Test known broken examples (should fail gracefully)
      run: |
        echo "Testing known broken examples (these should fail with clear errors)..."
        
        # List of examples with known issues
        BROKEN_EXAMPLES=(
          "demo.noo"
          "type_system_demo.noo"
          "constraints_demo.noo"
          "trait_system_demo.noo"
          "generic_safe_thrush_demo.noo"
          "trait_truly_multiline_demo.noo"
        )
        
        for example in "${BROKEN_EXAMPLES[@]}"; do
          echo "Testing examples/$example (expecting failure)..."
          if npm start "examples/$example" 2>/dev/null; then
            echo "⚠️ $example unexpectedly passed (this might be good news!)"
          else
            echo "✅ $example failed as expected"
          fi
        done
        
                 echo "✅ Broken examples validation completed"
    
    - name: Generate validation summary
      if: always()
      run: |
                 echo "## Example Validation Results" >> $GITHUB_STEP_SUMMARY
         echo "" >> $GITHUB_STEP_SUMMARY
         echo "⚠️ **Note**: Example validation is currently informational only due to known type system issues." >> $GITHUB_STEP_SUMMARY
         echo "See [Language Weaknesses](docs/LANGUAGE_WEAKNESSES.md) for details and fix priorities." >> $GITHUB_STEP_SUMMARY
         echo "" >> $GITHUB_STEP_SUMMARY
         echo "### Working Examples (should pass)" >> $GITHUB_STEP_SUMMARY
        echo "- basic.noo" >> $GITHUB_STEP_SUMMARY
        echo "- adt_demo.noo" >> $GITHUB_STEP_SUMMARY
        echo "- safe_thrush_demo.noo" >> $GITHUB_STEP_SUMMARY
        echo "- simple_adt.noo" >> $GITHUB_STEP_SUMMARY
        echo "- minimal_trait_test.noo" >> $GITHUB_STEP_SUMMARY
        echo "- math_functions.noo" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Known Issues (expected to fail)" >> $GITHUB_STEP_SUMMARY
        echo "See [Language Weaknesses](docs/LANGUAGE_WEAKNESSES.md) for details:" >> $GITHUB_STEP_SUMMARY
        echo "- demo.noo - Generic ADT constructor issues" >> $GITHUB_STEP_SUMMARY
        echo "- type_system_demo.noo - Type annotation issues" >> $GITHUB_STEP_SUMMARY
        echo "- constraints_demo.noo - Variable definition issues" >> $GITHUB_STEP_SUMMARY
        echo "- trait_system_demo.noo - Trait function constraint issues" >> $GITHUB_STEP_SUMMARY
        echo "- generic_safe_thrush_demo.noo - Safe thrush operator with Result monad" >> $GITHUB_STEP_SUMMARY
        echo "- trait_truly_multiline_demo.noo - Type mismatch errors" >> $GITHUB_STEP_SUMMARY
    
    - name: Upload example test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: example-validation-results
        path: |
          examples/
        retention-days: 14

  # Run benchmarks (can run in parallel with other tests)
  benchmarks:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Run core benchmarks
      run: |
        npm run benchmark
        # Validate that benchmark results were generated
        if [ ! -d "benchmark-results" ] || [ -z "$(find benchmark-results -name '*.json' 2>/dev/null)" ]; then
          echo "::error::Core benchmarks failed to generate results"
          exit 1
        fi
        echo "✅ Core benchmarks completed successfully"
      
    - name: Run REPL benchmarks (allowed to fail)
      run: |
        set +e  # Don't exit on error for REPL benchmarks
        npm run benchmark:repl
        REPL_EXIT_CODE=$?
        
        if [ $REPL_EXIT_CODE -eq 0 ]; then
          echo "✅ REPL benchmarks completed successfully"
          echo "REPL_STATUS=success" >> $GITHUB_ENV
        else
          echo "⚠️ REPL benchmarks failed - this is not blocking"
          echo "::warning::REPL benchmarks failed but pipeline continues"
          echo "REPL_STATUS=failed" >> $GITHUB_ENV
        fi
        
        set -e  # Re-enable exit on error
    
    - name: Validate benchmark results
      run: |
        # Check core benchmark results exist and are valid
        CORE_RESULTS=$(find benchmark-results -name "results-*.json" 2>/dev/null)
        if [ -z "$CORE_RESULTS" ]; then
          echo "::error::No core benchmark results found"
          exit 1
        fi
        
        # Validate JSON structure
        for file in $CORE_RESULTS; do
          if ! jq empty "$file" 2>/dev/null; then
            echo "::error::Invalid JSON in benchmark file: $file"
            exit 1
          fi
          
          # Check for required fields
          if ! jq -e '.results | length > 0' "$file" >/dev/null; then
            echo "::error::No benchmark results in file: $file"
            exit 1
          fi
        done
        
        echo "✅ Benchmark validation passed"
        
        # Add results summary to job
        echo "## Benchmark Results" >> $GITHUB_STEP_SUMMARY
        echo "| Benchmark | Phase | Min | Max | Avg | Median |" >> $GITHUB_STEP_SUMMARY
        echo "|-----------|-------|-----|-----|-----|--------|" >> $GITHUB_STEP_SUMMARY
        
        # Extract and display results
        for file in $CORE_RESULTS; do
          jq -r '.results[] | "| \(.benchmark) | \(.phase) | \(.min)ms | \(.max)ms | \(.avg)ms | \(.median)ms |"' "$file" >> $GITHUB_STEP_SUMMARY
        done
        
        if [ "$REPL_STATUS" = "failed" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "⚠️ **Note**: REPL benchmarks failed and were skipped" >> $GITHUB_STEP_SUMMARY
        fi
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: |
          benchmark-results/
        retention-days: 90

  # Performance comparison for PRs only
  performance-comparison:
    runs-on: ubuntu-latest
    # Only run for pull requests, not for pushes to branches
    if: github.event_name == 'pull_request' && github.base_ref == 'main'
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
      
    - name: Cache Jest transform cache
      uses: actions/cache@v4
      with:
        path: .jest-cache
        key: jest-cache-${{ runner.os }}-${{ hashFiles('**/package-lock.json') }}-${{ hashFiles('src/**/*.ts', 'test/**/*.ts') }}
        restore-keys: |
          jest-cache-${{ runner.os }}-${{ hashFiles('**/package-lock.json') }}-
          jest-cache-${{ runner.os }}-
      
    - name: Build and benchmark current branch
      run: |
        echo "Building and benchmarking current branch..."
        if npm run build; then
          echo "✅ Current branch build successful"
          if npm run benchmark:all; then
            echo "✅ Current branch benchmarks successful"
            if [ -d "benchmark-results" ]; then
              mv benchmark-results benchmark-results-current
              echo "✅ Current branch results saved"
            else
              echo "⚠️ No benchmark results from current branch"
            fi
          else
            echo "⚠️ Current branch benchmarks failed"
          fi
        else
          echo "❌ Current branch build failed - skipping benchmarks"
        fi
    
    - name: Checkout and benchmark main branch
      run: |
        echo "Switching to main branch..."
        git reset --hard HEAD
        git clean -fd
        git checkout origin/main
        
        echo "Installing dependencies for main branch..."
        if npm ci; then
          echo "✅ Main branch dependencies installed"
          
          echo "Building main branch..."
          if npm run build; then
            echo "✅ Main branch build successful"
            
            echo "Benchmarking main branch..."
            if npm run benchmark:all; then
              echo "✅ Main branch benchmarks successful"
              if [ -d "benchmark-results" ]; then
                mv benchmark-results benchmark-results-main
                echo "✅ Main branch results saved"
              else
                echo "⚠️ No benchmark results from main branch"
              fi
            else
              echo "⚠️ Main branch benchmarks failed"
            fi
          else
            echo "❌ Main branch build failed - skipping benchmarks"
            echo "This might be due to test file issues on main branch"
          fi
        else
          echo "❌ Main branch dependency installation failed"
        fi
    
    - name: Compare performance
      run: |
        echo "## Performance Comparison" >> $GITHUB_STEP_SUMMARY
        echo "Comparing current branch against main..." >> $GITHUB_STEP_SUMMARY
        
        # Check if benchmark files exist with proper globbing
        CURRENT_RESULTS=""
        MAIN_RESULTS=""
        
        if [ -d "benchmark-results-current" ]; then
          CURRENT_RESULTS=$(find benchmark-results-current -name "*.json" 2>/dev/null | head -1)
        fi
        
        if [ -d "benchmark-results-main" ]; then
          MAIN_RESULTS=$(find benchmark-results-main -name "*.json" 2>/dev/null | head -1)
        fi
        
        if [ -n "$CURRENT_RESULTS" ] && [ -n "$MAIN_RESULTS" ]; then
          echo "✅ Benchmark files found for both branches" >> $GITHUB_STEP_SUMMARY
          
          # Add basic performance validation
          echo "### Performance Analysis" >> $GITHUB_STEP_SUMMARY
          
          # Extract performance metrics and compare
          CURRENT_AVG=$(grep -o '"avg":[0-9.]*' "$CURRENT_RESULTS" | head -1 | cut -d':' -f2)
          MAIN_AVG=$(grep -o '"avg":[0-9.]*' "$MAIN_RESULTS" | head -1 | cut -d':' -f2)
          
          if [ -n "$CURRENT_AVG" ] && [ -n "$MAIN_AVG" ]; then
            echo "- Current branch average: ${CURRENT_AVG}ms" >> $GITHUB_STEP_SUMMARY
            echo "- Main branch average: ${MAIN_AVG}ms" >> $GITHUB_STEP_SUMMARY
            
            # Calculate percentage difference (using awk for floating point)
            DIFF_PERCENT=$(awk "BEGIN {printf \"%.1f\", (($CURRENT_AVG - $MAIN_AVG) / $MAIN_AVG) * 100}")
            echo "- Performance difference: ${DIFF_PERCENT}%" >> $GITHUB_STEP_SUMMARY
            
            # Flag significant regressions
            if awk "BEGIN {exit ($DIFF_PERCENT > 20)}"; then
              echo "⚠️ **WARNING: Performance regression detected (>20% slower)**" >> $GITHUB_STEP_SUMMARY
              echo "::warning::Performance regression: ${DIFF_PERCENT}% slower than main"
            elif awk "BEGIN {exit ($DIFF_PERCENT < -10)}"; then
              echo "🚀 **Performance improvement detected (>10% faster)**" >> $GITHUB_STEP_SUMMARY
            else
              echo "✅ Performance within acceptable range" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "⚠️ Could not extract performance metrics for comparison" >> $GITHUB_STEP_SUMMARY
          fi
        elif [ -n "$CURRENT_RESULTS" ]; then
          echo "⚠️ Only current branch benchmarks available" >> $GITHUB_STEP_SUMMARY
          echo "Main branch benchmarks failed (possibly due to build issues)" >> $GITHUB_STEP_SUMMARY
        elif [ -n "$MAIN_RESULTS" ]; then
          echo "⚠️ Only main branch benchmarks available" >> $GITHUB_STEP_SUMMARY
          echo "Current branch benchmarks failed" >> $GITHUB_STEP_SUMMARY
        else
          echo "⚠️ No benchmark results available for comparison" >> $GITHUB_STEP_SUMMARY
          echo "Both branches had build or benchmark issues" >> $GITHUB_STEP_SUMMARY
        fi
    
    - name: Upload comparison results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-comparison
        path: |
          benchmark-results-current/
          benchmark-results-main/
        retention-days: 30