{"version":3,"sources":["/workspace/test/lexer.test.ts"],"sourcesContent":["import { Lexer } from \"../src/lexer\";\n\ndescribe(\"Lexer\", () => {\n  // Helper function to create lexer and get all tokens\n  const tokenize = (input: string) => new Lexer(input).tokenize();\n  \n  // Helper function to get token values without location info\n  const getTokenValues = (input: string) => \n    tokenize(input).map(token => ({ type: token.type, value: token.value }));\n\n  describe(\"Numbers\", () => {\n    test(\"should tokenize integers\", () => {\n      const tokens = getTokenValues(\"123\");\n      expect(tokens).toEqual([\n        { type: \"NUMBER\", value: \"123\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should tokenize floating point numbers\", () => {\n      const tokens = getTokenValues(\"123.456\");\n      expect(tokens).toEqual([\n        { type: \"NUMBER\", value: \"123.456\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should tokenize number followed by non-digit\", () => {\n      const tokens = getTokenValues(\"123abc\");\n      expect(tokens).toEqual([\n        { type: \"NUMBER\", value: \"123\" },\n        { type: \"IDENTIFIER\", value: \"abc\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should not tokenize dot without following digit as float\", () => {\n      const tokens = getTokenValues(\"123.\");\n      expect(tokens).toEqual([\n        { type: \"NUMBER\", value: \"123\" },\n        { type: \"PUNCTUATION\", value: \".\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n  });\n\n  describe(\"Strings\", () => {\n    test(\"should tokenize double-quoted strings\", () => {\n      const tokens = getTokenValues('\"hello world\"');\n      expect(tokens).toEqual([\n        { type: \"STRING\", value: \"hello world\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should tokenize single-quoted strings\", () => {\n      const tokens = getTokenValues(\"'hello world'\");\n      expect(tokens).toEqual([\n        { type: \"STRING\", value: \"hello world\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle escaped characters in strings\", () => {\n      const tokens = getTokenValues('\"hello \\\\\"world\\\\\"\"');\n      expect(tokens).toEqual([\n        { type: \"STRING\", value: 'hello \"world\"' },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle unclosed strings\", () => {\n      const tokens = getTokenValues('\"hello');\n      expect(tokens).toEqual([\n        { type: \"STRING\", value: \"hello\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle escaped backslash at end of string\", () => {\n      const tokens = getTokenValues('\"hello\\\\\\\\\"');\n      expect(tokens).toEqual([\n        { type: \"STRING\", value: \"hello\\\\\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle escape sequence at end of input\", () => {\n      const tokens = getTokenValues('\"hello\\\\');\n      expect(tokens).toEqual([\n        { type: \"STRING\", value: \"hello\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n  });\n\n  describe(\"Identifiers and Keywords\", () => {\n    test(\"should tokenize basic identifiers\", () => {\n      const tokens = getTokenValues(\"variable\");\n      expect(tokens).toEqual([\n        { type: \"IDENTIFIER\", value: \"variable\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should tokenize identifiers with underscores and numbers\", () => {\n      const tokens = getTokenValues(\"var_123\");\n      expect(tokens).toEqual([\n        { type: \"IDENTIFIER\", value: \"var_123\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should recognize keywords\", () => {\n      const keywords = [\"if\", \"then\", \"else\", \"let\", \"in\", \"fn\", \"import\", \"mut\", \"where\", \"type\", \"match\", \"with\", \"given\", \"is\", \"and\", \"or\", \"implements\", \"constraint\", \"implement\"];\n      \n      for (const keyword of keywords) {\n        const tokens = getTokenValues(keyword);\n        expect(tokens).toEqual([\n          { type: \"KEYWORD\", value: keyword },\n          { type: \"EOF\", value: \"\" }\n        ]);\n      }\n    });\n\n    test(\"should recognize primitive type keywords\", () => {\n      const primitives = [\"Int\", \"Number\", \"String\", \"Unit\", \"List\"];\n      \n      for (const primitive of primitives) {\n        const tokens = getTokenValues(primitive);\n        expect(tokens).toEqual([\n          { type: \"KEYWORD\", value: primitive },\n          { type: \"EOF\", value: \"\" }\n        ]);\n      }\n    });\n\n    test(\"should handle mut! special case\", () => {\n      const tokens = getTokenValues(\"mut!\");\n      expect(tokens).toEqual([\n        { type: \"KEYWORD\", value: \"mut!\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle mut without exclamation\", () => {\n      const tokens = getTokenValues(\"mut\");\n      expect(tokens).toEqual([\n        { type: \"KEYWORD\", value: \"mut\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle identifiers starting with underscore\", () => {\n      const tokens = getTokenValues(\"_private\");\n      expect(tokens).toEqual([\n        { type: \"IDENTIFIER\", value: \"_private\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n  });\n\n  describe(\"Operators\", () => {\n    test(\"should tokenize multi-character operators\", () => {\n      const multiCharOps = [\"|>\", \"<|\", \"==\", \"!=\", \"<=\", \">=\", \"=>\", \"->\"];\n      \n      for (const op of multiCharOps) {\n        const tokens = getTokenValues(op);\n        expect(tokens).toEqual([\n          { type: \"OPERATOR\", value: op },\n          { type: \"EOF\", value: \"\" }\n        ]);\n      }\n    });\n\n    test(\"should tokenize single-character operators\", () => {\n      const singleCharOps = [\"+\", \"-\", \"*\", \"/\", \"<\", \">\", \"=\", \"|\", \"$\"];\n      \n      for (const op of singleCharOps) {\n        const tokens = getTokenValues(op);\n        expect(tokens).toEqual([\n          { type: \"OPERATOR\", value: op },\n          { type: \"EOF\", value: \"\" }\n        ]);\n      }\n    });\n\n    test(\"should prefer multi-character operators over single\", () => {\n      const tokens = getTokenValues(\"==\");\n      expect(tokens).toEqual([\n        { type: \"OPERATOR\", value: \"==\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle operators in sequence\", () => {\n      const tokens = getTokenValues(\"+-*/\");\n      expect(tokens).toEqual([\n        { type: \"OPERATOR\", value: \"+\" },\n        { type: \"OPERATOR\", value: \"-\" },\n        { type: \"OPERATOR\", value: \"*\" },\n        { type: \"OPERATOR\", value: \"/\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle single character operator fallback\", () => {\n      // Test the fallback case where no multi-character operator matches\n      // This specifically tests line 224 by using \"!\" which matches the regex but isn't in the multi-char list\n      const tokens = getTokenValues(\"!\");\n      expect(tokens).toEqual([\n        { type: \"OPERATOR\", value: \"!\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n  });\n\n  describe(\"Punctuation\", () => {\n    test(\"should tokenize punctuation characters\", () => {\n      const punctuation = [\"(\", \")\", \",\", \";\", \":\", \"[\", \"]\", \"{\", \"}\"];\n      \n      for (const punct of punctuation) {\n        const tokens = getTokenValues(punct);\n        expect(tokens).toEqual([\n          { type: \"PUNCTUATION\", value: punct },\n          { type: \"EOF\", value: \"\" }\n        ]);\n      }\n    });\n\n    test(\"should handle period as punctuation\", () => {\n      const tokens = getTokenValues(\".\");\n      expect(tokens).toEqual([\n        { type: \"PUNCTUATION\", value: \".\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n  });\n\n  describe(\"Accessors\", () => {\n    test(\"should tokenize basic accessor\", () => {\n      const tokens = getTokenValues(\"@field\");\n      expect(tokens).toEqual([\n        { type: \"ACCESSOR\", value: \"field\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should tokenize accessor with numbers and underscores\", () => {\n      const tokens = getTokenValues(\"@field_123\");\n      expect(tokens).toEqual([\n        { type: \"ACCESSOR\", value: \"field_123\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle @ without following identifier\", () => {\n      const tokens = getTokenValues(\"@\");\n      expect(tokens).toEqual([\n        { type: \"ACCESSOR\", value: \"\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle @ followed by non-identifier\", () => {\n      const tokens = getTokenValues(\"@(\");\n      expect(tokens).toEqual([\n        { type: \"ACCESSOR\", value: \"\" },\n        { type: \"PUNCTUATION\", value: \"(\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n  });\n\n  describe(\"Comments\", () => {\n    test(\"should skip single-line comments\", () => {\n      const codeWithComments = `\n        # this is a comment\n        x = 5 # inline comment\n        y = 10\n        # another comment\n        x + y # trailing comment\n      `;\n      const codeWithoutComments = `\n        x = 5\n        y = 10\n        x + y\n      `;\n      const tokensWithComments = new Lexer(codeWithComments).tokenize();\n      const tokensWithoutComments = new Lexer(codeWithoutComments).tokenize();\n      // Remove location info for comparison\n      const stripLoc = (t: any) => ({ type: t.type, value: t.value });\n      expect(tokensWithComments.map(stripLoc)).toEqual(\n        tokensWithoutComments.map(stripLoc),\n      );\n      // Ensure no COMMENT tokens are present\n      expect(tokensWithComments.some((t) => t.type === \"COMMENT\")).toBe(false);\n    });\n\n    test(\"should handle comment at end of file\", () => {\n      const tokens = getTokenValues(\"x # comment\");\n      expect(tokens).toEqual([\n        { type: \"IDENTIFIER\", value: \"x\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle multiple comments\", () => {\n      const tokens = getTokenValues(\"# comment1\\n# comment2\\nx\");\n      expect(tokens).toEqual([\n        { type: \"IDENTIFIER\", value: \"x\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle comment encountered in nextToken\", () => {\n      // This tests the comment handling path in nextToken (lines 317-319)\n      const lexer = new Lexer(\"# comment\\n\");\n      const token = lexer.nextToken();\n      expect(token.type).toBe(\"EOF\");\n    });\n  });\n\n  describe(\"Whitespace handling\", () => {\n    test(\"should skip whitespace\", () => {\n      const tokens = getTokenValues(\"  \\t  x  \\n  y  \");\n      expect(tokens).toEqual([\n        { type: \"IDENTIFIER\", value: \"x\" },\n        { type: \"IDENTIFIER\", value: \"y\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle empty input\", () => {\n      const tokens = getTokenValues(\"\");\n      expect(tokens).toEqual([\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle whitespace only\", () => {\n      const tokens = getTokenValues(\"   \\t\\n  \");\n      expect(tokens).toEqual([\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n  });\n\n  describe(\"Unknown characters\", () => {\n    test(\"should handle unknown characters as punctuation\", () => {\n      const tokens = getTokenValues(\"~\");\n      expect(tokens).toEqual([\n        { type: \"PUNCTUATION\", value: \"~\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle unknown characters that are whitespace\", () => {\n      // Test with a Unicode whitespace character\n      const tokens = getTokenValues(\"x\\u00A0y\"); // Non-breaking space\n      expect(tokens).toEqual([\n        { type: \"IDENTIFIER\", value: \"x\" },\n        { type: \"IDENTIFIER\", value: \"y\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle unknown whitespace characters in nextToken path\", () => {\n      // This tests line 327 - when unknown character is whitespace and triggers recursive nextToken\n      const lexer = new Lexer(\"\\u00A0\"); // Non-breaking space as unknown character\n      const token = lexer.nextToken();\n      expect(token.type).toBe(\"EOF\");\n    });\n  });\n\n  describe(\"Edge cases for 100% coverage\", () => {\n    test(\"should handle comment in nextToken path (lines 317-319)\", () => {\n      // This is tricky - we need a scenario where skipWhitespace doesn't handle the comment\n      // Let's create a scenario where the lexer position is at a comment after other processing\n      const lexer = new Lexer(\"x#comment\");\n      \n      // Get first token (x)\n      const firstToken = lexer.nextToken();\n      expect(firstToken.type).toBe(\"IDENTIFIER\");\n      \n      // Now position should be at the comment, and nextToken should handle it\n      const secondToken = lexer.nextToken();\n      expect(secondToken.type).toBe(\"EOF\");\n    });\n\n    test(\"should handle whitespace in unknown character path (line 327)\", () => {\n      // Create a test where an unknown character becomes whitespace after advance()\n      // This happens when we have a character that doesn't match any category initially\n      // but when advanced and checked again, is whitespace\n      \n      // Use a Unicode character that might be treated as unknown initially\n      const lexer = new Lexer(\"\\u2000\"); // EN QUAD - Unicode space\n      const token = lexer.nextToken();\n      expect(token.type).toBe(\"EOF\");\n    });\n\n    test(\"should handle form feed as potential unknown whitespace\", () => {\n      // Form feed (\\f) might trigger the unknown character path in some cases\n      const lexer = new Lexer(\"\\fx\");\n      const token = lexer.nextToken();\n      expect(token.type).toBe(\"IDENTIFIER\");\n      expect(token.value).toBe(\"x\");\n    });\n\n    test(\"should handle zero-width space as unknown character\", () => {\n      // Zero-width characters are treated as punctuation, not whitespace by the lexer\n      const lexer = new Lexer(\"\\u200B\\u200C\\u200Dx\"); // Various zero-width characters\n      const token = lexer.nextToken();\n      expect(token.type).toBe(\"PUNCTUATION\");\n      expect(token.value).toBe(\"\\u200B\");\n    });\n\n    test(\"should handle tab character in unknown path (line 327)\", () => {\n      // This test specifically targets line 327 - unknown character that becomes whitespace\n      // We need a character that doesn't match initial patterns but is whitespace\n      // Let's try a form feed character or vertical tab that might slip through\n      const lexer = new Lexer(\"\\v\\fx\"); // vertical tab and form feed\n      const tokens = tokenize(\"\\v\\fx\");\n      expect(tokens[0].type).toBe(\"IDENTIFIER\");\n      expect(tokens[0].value).toBe(\"x\");\n    });\n\n    test(\"should handle specific Unicode whitespace that might be unknown initially\", () => {\n      // Test with various Unicode whitespace characters that might not match initial \\s\n      const characters = [\n        \"\\u00A0\", // Non-breaking space\n        \"\\u1680\", // Ogham space mark\n        \"\\u2000\", // En quad\n        \"\\u2001\", // Em quad\n        \"\\u2002\", // En space\n        \"\\u2003\", // Em space\n        \"\\u2004\", // Three-per-em space\n        \"\\u2005\", // Four-per-em space\n        \"\\u2006\", // Six-per-em space\n        \"\\u2007\", // Figure space\n        \"\\u2008\", // Punctuation space\n        \"\\u2009\", // Thin space\n        \"\\u200A\", // Hair space\n        \"\\u202F\", // Narrow no-break space\n        \"\\u205F\", // Medium mathematical space\n        \"\\u3000\", // Ideographic space\n      ];\n\n      for (const char of characters) {\n        const lexer = new Lexer(char + \"x\");\n        const token = lexer.nextToken();\n        expect(token.type).toBe(\"IDENTIFIER\");\n        expect(token.value).toBe(\"x\");\n      }\n    });\n\n    test(\"should trigger comment fallback in nextToken (lines 317-319)\", () => {\n      // Try to create a scenario where skipWhitespace doesn't handle the comment\n      // This is a very specific edge case - create a lexer where we manually position\n      // it so that skipWhitespace has already been called but a comment appears\n      const input = \"a\\t#comment\";\n      const lexer = new Lexer(input);\n      \n      // Get the 'a' token\n      const firstToken = lexer.nextToken();\n      expect(firstToken.type).toBe(\"IDENTIFIER\");\n      expect(firstToken.value).toBe(\"a\");\n      \n      // The next token should skip the tab and handle the comment\n      const secondToken = lexer.nextToken();\n      expect(secondToken.type).toBe(\"EOF\");\n    });\n\n    test(\"should trigger unknown whitespace path (line 327) with non-breaking space\", () => {\n      // Use a non-breaking space which might not be caught by initial whitespace checks\n      const input = \"\\u00A0x\"; // Non-breaking space followed by identifier\n      const tokens = tokenize(input);\n      expect(tokens[0].type).toBe(\"IDENTIFIER\");\n      expect(tokens[0].value).toBe(\"x\");\n    });\n\n    test(\"should trigger unknown whitespace path (line 327) with exotic whitespace\", () => {\n      // Try other Unicode whitespace characters that might not match initial /\\s/\n      const input = \"\\u2000\\u2001\\u2002x\"; // En quad, Em quad, En space\n      const tokens = tokenize(input);\n      expect(tokens[0].type).toBe(\"IDENTIFIER\");\n      expect(tokens[0].value).toBe(\"x\");\n    });\n\n    test(\"should trigger exact uncovered paths with null character edge case\", () => {\n      // Try a null character that might behave unexpectedly\n      const input = \"\\0x\";\n      const tokens = tokenize(input);\n      // This should either handle the null as punctuation or skip it\n      expect(tokens.length).toBeGreaterThan(0);\n    });\n\n    test(\"should handle character that looks like operator but isn't\", () => {\n      // Try to trigger the single character operator fallback (line 224)\n      // Use a character that matches operator regex but isn't multi-char\n      const input = \"!x\"; // ! is in the operator regex and not multi-char in this context\n      const tokens = tokenize(input);\n      expect(tokens[0].type).toBe(\"OPERATOR\");\n      expect(tokens[0].value).toBe(\"!\");\n      expect(tokens[1].type).toBe(\"IDENTIFIER\");\n      expect(tokens[1].value).toBe(\"x\");\n    });\n\n    test(\"should handle comment immediately after EOF check\", () => {\n      // Try to create a scenario where comment handling hits the nextToken path\n      const input = \"#\";\n      const tokens = tokenize(input);\n      expect(tokens[0].type).toBe(\"EOF\");\n    });\n\n    test(\"should handle edge case for exact line coverage - carriage return before comment\", () => {\n      // Try using carriage return which might not be handled the same as other whitespace\n      const input = \"\\r#comment\\nx\";\n      const tokens = tokenize(input);\n      expect(tokens[0].type).toBe(\"IDENTIFIER\");\n      expect(tokens[0].value).toBe(\"x\");\n    });\n\n    test(\"should handle zero-width joiner that might not match \\\\s regex\", () => {\n      // Zero-width joiner (U+200D) might not match \\\\s but could be whitespace-like\n      const input = \"\\u200Dx\";\n      const tokens = tokenize(input);\n      // This should either skip the ZWJJ or treat it as punctuation\n      if (tokens[0].type === \"IDENTIFIER\") {\n        expect(tokens[0].value).toBe(\"x\");\n      } else {\n        expect(tokens[0].type).toBe(\"PUNCTUATION\");\n      }\n    });\n  });\n\n  describe(\"Line and column tracking\", () => {\n    test(\"should track line and column positions\", () => {\n      const lexer = new Lexer(\"x\\ny\");\n      const tokens = lexer.tokenize();\n      \n      expect(tokens[0].location.start.line).toBe(1);\n      expect(tokens[0].location.start.column).toBe(1);\n      expect(tokens[0].location.end.line).toBe(1);\n      expect(tokens[0].location.end.column).toBe(2);\n      \n      expect(tokens[1].location.start.line).toBe(2);\n      expect(tokens[1].location.start.column).toBe(1);\n      expect(tokens[1].location.end.line).toBe(2);\n      expect(tokens[1].location.end.column).toBe(2);\n    });\n\n    test(\"should handle column advancement\", () => {\n      const lexer = new Lexer(\"abc\");\n      const tokens = lexer.tokenize();\n      \n      expect(tokens[0].location.start.line).toBe(1);\n      expect(tokens[0].location.start.column).toBe(1);\n      expect(tokens[0].location.end.line).toBe(1);\n      expect(tokens[0].location.end.column).toBe(4);\n    });\n  });\n\n  describe(\"Complex expressions\", () => {\n    test(\"should tokenize complex expression\", () => {\n      const tokens = getTokenValues('fn add(x, y) -> x + y\\nlet result = add(1, 2)');\n      expect(tokens).toEqual([\n        { type: \"KEYWORD\", value: \"fn\" },\n        { type: \"IDENTIFIER\", value: \"add\" },\n        { type: \"PUNCTUATION\", value: \"(\" },\n        { type: \"IDENTIFIER\", value: \"x\" },\n        { type: \"PUNCTUATION\", value: \",\" },\n        { type: \"IDENTIFIER\", value: \"y\" },\n        { type: \"PUNCTUATION\", value: \")\" },\n        { type: \"OPERATOR\", value: \"->\" },\n        { type: \"IDENTIFIER\", value: \"x\" },\n        { type: \"OPERATOR\", value: \"+\" },\n        { type: \"IDENTIFIER\", value: \"y\" },\n        { type: \"KEYWORD\", value: \"let\" },\n        { type: \"IDENTIFIER\", value: \"result\" },\n        { type: \"OPERATOR\", value: \"=\" },\n        { type: \"IDENTIFIER\", value: \"add\" },\n        { type: \"PUNCTUATION\", value: \"(\" },\n        { type: \"NUMBER\", value: \"1\" },\n        { type: \"PUNCTUATION\", value: \",\" },\n        { type: \"NUMBER\", value: \"2\" },\n        { type: \"PUNCTUATION\", value: \")\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle mixed operators and punctuation\", () => {\n      const tokens = getTokenValues(\"(x == y) && z\");\n      expect(tokens).toEqual([\n        { type: \"PUNCTUATION\", value: \"(\" },\n        { type: \"IDENTIFIER\", value: \"x\" },\n        { type: \"OPERATOR\", value: \"==\" },\n        { type: \"IDENTIFIER\", value: \"y\" },\n        { type: \"PUNCTUATION\", value: \")\" },\n        { type: \"PUNCTUATION\", value: \"&\" },\n        { type: \"PUNCTUATION\", value: \"&\" },\n        { type: \"IDENTIFIER\", value: \"z\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n  });\n\n  describe(\"Edge cases\", () => {\n    test(\"should handle EOF conditions\", () => {\n      const lexer = new Lexer(\"\");\n      const token = lexer.nextToken();\n      expect(token.type).toBe(\"EOF\");\n      expect(token.value).toBe(\"\");\n    });\n\n    test(\"should handle sequential whitespace and comments\", () => {\n      const tokens = getTokenValues(\"  # comment\\n  \\t# another\\n x\");\n      expect(tokens).toEqual([\n        { type: \"IDENTIFIER\", value: \"x\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n\n    test(\"should handle operators at end of input\", () => {\n      const tokens = getTokenValues(\"x +\");\n      expect(tokens).toEqual([\n        { type: \"IDENTIFIER\", value: \"x\" },\n        { type: \"OPERATOR\", value: \"+\" },\n        { type: \"EOF\", value: \"\" }\n      ]);\n    });\n  });\n});\n"],"names":["describe","tokenize","input","Lexer","getTokenValues","map","token","type","value","test","tokens","expect","toEqual","keywords","keyword","primitives","primitive","multiCharOps","op","singleCharOps","punctuation","punct","codeWithComments","codeWithoutComments","tokensWithComments","tokensWithoutComments","stripLoc","t","some","toBe","lexer","nextToken","firstToken","secondToken","characters","char","length","toBeGreaterThan","location","start","line","column","end"],"mappings":";;;;uBAAsB;AAEtBA,SAAS,SAAS;IAChB,qDAAqD;IACrD,MAAMC,WAAW,CAACC,QAAkB,IAAIC,YAAK,CAACD,OAAOD,QAAQ;IAE7D,4DAA4D;IAC5D,MAAMG,iBAAiB,CAACF,QACtBD,SAASC,OAAOG,GAAG,CAACC,CAAAA,QAAU,CAAA;gBAAEC,MAAMD,MAAMC,IAAI;gBAAEC,OAAOF,MAAME,KAAK;YAAC,CAAA;IAEvER,SAAS,WAAW;QAClBS,KAAK,4BAA4B;YAC/B,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAUC,OAAO;gBAAM;gBAC/B;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,0CAA0C;YAC7C,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAUC,OAAO;gBAAU;gBACnC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,gDAAgD;YACnD,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAUC,OAAO;gBAAM;gBAC/B;oBAAED,MAAM;oBAAcC,OAAO;gBAAM;gBACnC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,4DAA4D;YAC/D,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAUC,OAAO;gBAAM;gBAC/B;oBAAED,MAAM;oBAAeC,OAAO;gBAAI;gBAClC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;IACF;IAEAR,SAAS,WAAW;QAClBS,KAAK,yCAAyC;YAC5C,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAUC,OAAO;gBAAc;gBACvC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,yCAAyC;YAC5C,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAUC,OAAO;gBAAc;gBACvC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,+CAA+C;YAClD,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAUC,OAAO;gBAAgB;gBACzC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,kCAAkC;YACrC,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAUC,OAAO;gBAAQ;gBACjC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,oDAAoD;YACvD,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAUC,OAAO;gBAAU;gBACnC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,iDAAiD;YACpD,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAUC,OAAO;gBAAQ;gBACjC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;IACF;IAEAR,SAAS,4BAA4B;QACnCS,KAAK,qCAAqC;YACxC,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAcC,OAAO;gBAAW;gBACxC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,4DAA4D;YAC/D,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAcC,OAAO;gBAAU;gBACvC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,6BAA6B;YAChC,MAAMI,WAAW;gBAAC;gBAAM;gBAAQ;gBAAQ;gBAAO;gBAAM;gBAAM;gBAAU;gBAAO;gBAAS;gBAAQ;gBAAS;gBAAQ;gBAAS;gBAAM;gBAAO;gBAAM;gBAAc;gBAAc;aAAY;YAElL,KAAK,MAAMC,WAAWD,SAAU;gBAC9B,MAAMH,SAASN,eAAeU;gBAC9BH,OAAOD,QAAQE,OAAO,CAAC;oBACrB;wBAAEL,MAAM;wBAAWC,OAAOM;oBAAQ;oBAClC;wBAAEP,MAAM;wBAAOC,OAAO;oBAAG;iBAC1B;YACH;QACF;QAEAC,KAAK,4CAA4C;YAC/C,MAAMM,aAAa;gBAAC;gBAAO;gBAAU;gBAAU;gBAAQ;aAAO;YAE9D,KAAK,MAAMC,aAAaD,WAAY;gBAClC,MAAML,SAASN,eAAeY;gBAC9BL,OAAOD,QAAQE,OAAO,CAAC;oBACrB;wBAAEL,MAAM;wBAAWC,OAAOQ;oBAAU;oBACpC;wBAAET,MAAM;wBAAOC,OAAO;oBAAG;iBAC1B;YACH;QACF;QAEAC,KAAK,mCAAmC;YACtC,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAWC,OAAO;gBAAO;gBACjC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,yCAAyC;YAC5C,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAWC,OAAO;gBAAM;gBAChC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,sDAAsD;YACzD,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAcC,OAAO;gBAAW;gBACxC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;IACF;IAEAR,SAAS,aAAa;QACpBS,KAAK,6CAA6C;YAChD,MAAMQ,eAAe;gBAAC;gBAAM;gBAAM;gBAAM;gBAAM;gBAAM;gBAAM;gBAAM;aAAK;YAErE,KAAK,MAAMC,MAAMD,aAAc;gBAC7B,MAAMP,SAASN,eAAec;gBAC9BP,OAAOD,QAAQE,OAAO,CAAC;oBACrB;wBAAEL,MAAM;wBAAYC,OAAOU;oBAAG;oBAC9B;wBAAEX,MAAM;wBAAOC,OAAO;oBAAG;iBAC1B;YACH;QACF;QAEAC,KAAK,8CAA8C;YACjD,MAAMU,gBAAgB;gBAAC;gBAAK;gBAAK;gBAAK;gBAAK;gBAAK;gBAAK;gBAAK;gBAAK;aAAI;YAEnE,KAAK,MAAMD,MAAMC,cAAe;gBAC9B,MAAMT,SAASN,eAAec;gBAC9BP,OAAOD,QAAQE,OAAO,CAAC;oBACrB;wBAAEL,MAAM;wBAAYC,OAAOU;oBAAG;oBAC9B;wBAAEX,MAAM;wBAAOC,OAAO;oBAAG;iBAC1B;YACH;QACF;QAEAC,KAAK,uDAAuD;YAC1D,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAYC,OAAO;gBAAK;gBAChC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,uCAAuC;YAC1C,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAYC,OAAO;gBAAI;gBAC/B;oBAAED,MAAM;oBAAYC,OAAO;gBAAI;gBAC/B;oBAAED,MAAM;oBAAYC,OAAO;gBAAI;gBAC/B;oBAAED,MAAM;oBAAYC,OAAO;gBAAI;gBAC/B;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,oDAAoD;YACvD,mEAAmE;YACnE,yGAAyG;YACzG,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAYC,OAAO;gBAAI;gBAC/B;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;IACF;IAEAR,SAAS,eAAe;QACtBS,KAAK,0CAA0C;YAC7C,MAAMW,cAAc;gBAAC;gBAAK;gBAAK;gBAAK;gBAAK;gBAAK;gBAAK;gBAAK;gBAAK;aAAI;YAEjE,KAAK,MAAMC,SAASD,YAAa;gBAC/B,MAAMV,SAASN,eAAeiB;gBAC9BV,OAAOD,QAAQE,OAAO,CAAC;oBACrB;wBAAEL,MAAM;wBAAeC,OAAOa;oBAAM;oBACpC;wBAAEd,MAAM;wBAAOC,OAAO;oBAAG;iBAC1B;YACH;QACF;QAEAC,KAAK,uCAAuC;YAC1C,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAeC,OAAO;gBAAI;gBAClC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;IACF;IAEAR,SAAS,aAAa;QACpBS,KAAK,kCAAkC;YACrC,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAYC,OAAO;gBAAQ;gBACnC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,yDAAyD;YAC5D,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAYC,OAAO;gBAAY;gBACvC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,gDAAgD;YACnD,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAYC,OAAO;gBAAG;gBAC9B;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,8CAA8C;YACjD,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAYC,OAAO;gBAAG;gBAC9B;oBAAED,MAAM;oBAAeC,OAAO;gBAAI;gBAClC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;IACF;IAEAR,SAAS,YAAY;QACnBS,KAAK,oCAAoC;YACvC,MAAMa,mBAAmB,CAAC;;;;;;MAM1B,CAAC;YACD,MAAMC,sBAAsB,CAAC;;;;MAI7B,CAAC;YACD,MAAMC,qBAAqB,IAAIrB,YAAK,CAACmB,kBAAkBrB,QAAQ;YAC/D,MAAMwB,wBAAwB,IAAItB,YAAK,CAACoB,qBAAqBtB,QAAQ;YACrE,sCAAsC;YACtC,MAAMyB,WAAW,CAACC,IAAY,CAAA;oBAAEpB,MAAMoB,EAAEpB,IAAI;oBAAEC,OAAOmB,EAAEnB,KAAK;gBAAC,CAAA;YAC7DG,OAAOa,mBAAmBnB,GAAG,CAACqB,WAAWd,OAAO,CAC9Ca,sBAAsBpB,GAAG,CAACqB;YAE5B,uCAAuC;YACvCf,OAAOa,mBAAmBI,IAAI,CAAC,CAACD,IAAMA,EAAEpB,IAAI,KAAK,YAAYsB,IAAI,CAAC;QACpE;QAEApB,KAAK,wCAAwC;YAC3C,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAcC,OAAO;gBAAI;gBACjC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,mCAAmC;YACtC,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAcC,OAAO;gBAAI;gBACjC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,kDAAkD;YACrD,oEAAoE;YACpE,MAAMqB,QAAQ,IAAI3B,YAAK,CAAC;YACxB,MAAMG,QAAQwB,MAAMC,SAAS;YAC7BpB,OAAOL,MAAMC,IAAI,EAAEsB,IAAI,CAAC;QAC1B;IACF;IAEA7B,SAAS,uBAAuB;QAC9BS,KAAK,0BAA0B;YAC7B,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAcC,OAAO;gBAAI;gBACjC;oBAAED,MAAM;oBAAcC,OAAO;gBAAI;gBACjC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,6BAA6B;YAChC,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,iCAAiC;YACpC,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;IACF;IAEAR,SAAS,sBAAsB;QAC7BS,KAAK,mDAAmD;YACtD,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAeC,OAAO;gBAAI;gBAClC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,wDAAwD;YAC3D,2CAA2C;YAC3C,MAAMC,SAASN,eAAe,aAAa,qBAAqB;YAChEO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAcC,OAAO;gBAAI;gBACjC;oBAAED,MAAM;oBAAcC,OAAO;gBAAI;gBACjC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,iEAAiE;YACpE,8FAA8F;YAC9F,MAAMqB,QAAQ,IAAI3B,YAAK,CAAC,WAAW,0CAA0C;YAC7E,MAAMG,QAAQwB,MAAMC,SAAS;YAC7BpB,OAAOL,MAAMC,IAAI,EAAEsB,IAAI,CAAC;QAC1B;IACF;IAEA7B,SAAS,gCAAgC;QACvCS,KAAK,2DAA2D;YAC9D,sFAAsF;YACtF,0FAA0F;YAC1F,MAAMqB,QAAQ,IAAI3B,YAAK,CAAC;YAExB,sBAAsB;YACtB,MAAM6B,aAAaF,MAAMC,SAAS;YAClCpB,OAAOqB,WAAWzB,IAAI,EAAEsB,IAAI,CAAC;YAE7B,wEAAwE;YACxE,MAAMI,cAAcH,MAAMC,SAAS;YACnCpB,OAAOsB,YAAY1B,IAAI,EAAEsB,IAAI,CAAC;QAChC;QAEApB,KAAK,iEAAiE;YACpE,8EAA8E;YAC9E,kFAAkF;YAClF,qDAAqD;YAErD,qEAAqE;YACrE,MAAMqB,QAAQ,IAAI3B,YAAK,CAAC,WAAW,0BAA0B;YAC7D,MAAMG,QAAQwB,MAAMC,SAAS;YAC7BpB,OAAOL,MAAMC,IAAI,EAAEsB,IAAI,CAAC;QAC1B;QAEApB,KAAK,2DAA2D;YAC9D,wEAAwE;YACxE,MAAMqB,QAAQ,IAAI3B,YAAK,CAAC;YACxB,MAAMG,QAAQwB,MAAMC,SAAS;YAC7BpB,OAAOL,MAAMC,IAAI,EAAEsB,IAAI,CAAC;YACxBlB,OAAOL,MAAME,KAAK,EAAEqB,IAAI,CAAC;QAC3B;QAEApB,KAAK,uDAAuD;YAC1D,gFAAgF;YAChF,MAAMqB,QAAQ,IAAI3B,YAAK,CAAC,wBAAwB,gCAAgC;YAChF,MAAMG,QAAQwB,MAAMC,SAAS;YAC7BpB,OAAOL,MAAMC,IAAI,EAAEsB,IAAI,CAAC;YACxBlB,OAAOL,MAAME,KAAK,EAAEqB,IAAI,CAAC;QAC3B;QAEApB,KAAK,0DAA0D;YAC7D,sFAAsF;YACtF,4EAA4E;YAC5E,0EAA0E;YAC1E,MAAMqB,QAAQ,IAAI3B,YAAK,CAAC,UAAU,6BAA6B;YAC/D,MAAMO,SAAST,SAAS;YACxBU,OAAOD,MAAM,CAAC,EAAE,CAACH,IAAI,EAAEsB,IAAI,CAAC;YAC5BlB,OAAOD,MAAM,CAAC,EAAE,CAACF,KAAK,EAAEqB,IAAI,CAAC;QAC/B;QAEApB,KAAK,6EAA6E;YAChF,kFAAkF;YAClF,MAAMyB,aAAa;gBACjB;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;aACD;YAED,KAAK,MAAMC,QAAQD,WAAY;gBAC7B,MAAMJ,QAAQ,IAAI3B,YAAK,CAACgC,OAAO;gBAC/B,MAAM7B,QAAQwB,MAAMC,SAAS;gBAC7BpB,OAAOL,MAAMC,IAAI,EAAEsB,IAAI,CAAC;gBACxBlB,OAAOL,MAAME,KAAK,EAAEqB,IAAI,CAAC;YAC3B;QACF;QAEApB,KAAK,gEAAgE;YACnE,2EAA2E;YAC3E,gFAAgF;YAChF,0EAA0E;YAC1E,MAAMP,QAAQ;YACd,MAAM4B,QAAQ,IAAI3B,YAAK,CAACD;YAExB,oBAAoB;YACpB,MAAM8B,aAAaF,MAAMC,SAAS;YAClCpB,OAAOqB,WAAWzB,IAAI,EAAEsB,IAAI,CAAC;YAC7BlB,OAAOqB,WAAWxB,KAAK,EAAEqB,IAAI,CAAC;YAE9B,4DAA4D;YAC5D,MAAMI,cAAcH,MAAMC,SAAS;YACnCpB,OAAOsB,YAAY1B,IAAI,EAAEsB,IAAI,CAAC;QAChC;QAEApB,KAAK,6EAA6E;YAChF,kFAAkF;YAClF,MAAMP,QAAQ,WAAW,4CAA4C;YACrE,MAAMQ,SAAST,SAASC;YACxBS,OAAOD,MAAM,CAAC,EAAE,CAACH,IAAI,EAAEsB,IAAI,CAAC;YAC5BlB,OAAOD,MAAM,CAAC,EAAE,CAACF,KAAK,EAAEqB,IAAI,CAAC;QAC/B;QAEApB,KAAK,4EAA4E;YAC/E,4EAA4E;YAC5E,MAAMP,QAAQ,uBAAuB,6BAA6B;YAClE,MAAMQ,SAAST,SAASC;YACxBS,OAAOD,MAAM,CAAC,EAAE,CAACH,IAAI,EAAEsB,IAAI,CAAC;YAC5BlB,OAAOD,MAAM,CAAC,EAAE,CAACF,KAAK,EAAEqB,IAAI,CAAC;QAC/B;QAEApB,KAAK,sEAAsE;YACzE,sDAAsD;YACtD,MAAMP,QAAQ;YACd,MAAMQ,SAAST,SAASC;YACxB,+DAA+D;YAC/DS,OAAOD,OAAO0B,MAAM,EAAEC,eAAe,CAAC;QACxC;QAEA5B,KAAK,8DAA8D;YACjE,mEAAmE;YACnE,mEAAmE;YACnE,MAAMP,QAAQ,MAAM,gEAAgE;YACpF,MAAMQ,SAAST,SAASC;YACxBS,OAAOD,MAAM,CAAC,EAAE,CAACH,IAAI,EAAEsB,IAAI,CAAC;YAC5BlB,OAAOD,MAAM,CAAC,EAAE,CAACF,KAAK,EAAEqB,IAAI,CAAC;YAC7BlB,OAAOD,MAAM,CAAC,EAAE,CAACH,IAAI,EAAEsB,IAAI,CAAC;YAC5BlB,OAAOD,MAAM,CAAC,EAAE,CAACF,KAAK,EAAEqB,IAAI,CAAC;QAC/B;QAEApB,KAAK,qDAAqD;YACxD,0EAA0E;YAC1E,MAAMP,QAAQ;YACd,MAAMQ,SAAST,SAASC;YACxBS,OAAOD,MAAM,CAAC,EAAE,CAACH,IAAI,EAAEsB,IAAI,CAAC;QAC9B;QAEApB,KAAK,oFAAoF;YACvF,oFAAoF;YACpF,MAAMP,QAAQ;YACd,MAAMQ,SAAST,SAASC;YACxBS,OAAOD,MAAM,CAAC,EAAE,CAACH,IAAI,EAAEsB,IAAI,CAAC;YAC5BlB,OAAOD,MAAM,CAAC,EAAE,CAACF,KAAK,EAAEqB,IAAI,CAAC;QAC/B;QAEApB,KAAK,kEAAkE;YACrE,8EAA8E;YAC9E,MAAMP,QAAQ;YACd,MAAMQ,SAAST,SAASC;YACxB,8DAA8D;YAC9D,IAAIQ,MAAM,CAAC,EAAE,CAACH,IAAI,KAAK,cAAc;gBACnCI,OAAOD,MAAM,CAAC,EAAE,CAACF,KAAK,EAAEqB,IAAI,CAAC;YAC/B,OAAO;gBACLlB,OAAOD,MAAM,CAAC,EAAE,CAACH,IAAI,EAAEsB,IAAI,CAAC;YAC9B;QACF;IACF;IAEA7B,SAAS,4BAA4B;QACnCS,KAAK,0CAA0C;YAC7C,MAAMqB,QAAQ,IAAI3B,YAAK,CAAC;YACxB,MAAMO,SAASoB,MAAM7B,QAAQ;YAE7BU,OAAOD,MAAM,CAAC,EAAE,CAAC4B,QAAQ,CAACC,KAAK,CAACC,IAAI,EAAEX,IAAI,CAAC;YAC3ClB,OAAOD,MAAM,CAAC,EAAE,CAAC4B,QAAQ,CAACC,KAAK,CAACE,MAAM,EAAEZ,IAAI,CAAC;YAC7ClB,OAAOD,MAAM,CAAC,EAAE,CAAC4B,QAAQ,CAACI,GAAG,CAACF,IAAI,EAAEX,IAAI,CAAC;YACzClB,OAAOD,MAAM,CAAC,EAAE,CAAC4B,QAAQ,CAACI,GAAG,CAACD,MAAM,EAAEZ,IAAI,CAAC;YAE3ClB,OAAOD,MAAM,CAAC,EAAE,CAAC4B,QAAQ,CAACC,KAAK,CAACC,IAAI,EAAEX,IAAI,CAAC;YAC3ClB,OAAOD,MAAM,CAAC,EAAE,CAAC4B,QAAQ,CAACC,KAAK,CAACE,MAAM,EAAEZ,IAAI,CAAC;YAC7ClB,OAAOD,MAAM,CAAC,EAAE,CAAC4B,QAAQ,CAACI,GAAG,CAACF,IAAI,EAAEX,IAAI,CAAC;YACzClB,OAAOD,MAAM,CAAC,EAAE,CAAC4B,QAAQ,CAACI,GAAG,CAACD,MAAM,EAAEZ,IAAI,CAAC;QAC7C;QAEApB,KAAK,oCAAoC;YACvC,MAAMqB,QAAQ,IAAI3B,YAAK,CAAC;YACxB,MAAMO,SAASoB,MAAM7B,QAAQ;YAE7BU,OAAOD,MAAM,CAAC,EAAE,CAAC4B,QAAQ,CAACC,KAAK,CAACC,IAAI,EAAEX,IAAI,CAAC;YAC3ClB,OAAOD,MAAM,CAAC,EAAE,CAAC4B,QAAQ,CAACC,KAAK,CAACE,MAAM,EAAEZ,IAAI,CAAC;YAC7ClB,OAAOD,MAAM,CAAC,EAAE,CAAC4B,QAAQ,CAACI,GAAG,CAACF,IAAI,EAAEX,IAAI,CAAC;YACzClB,OAAOD,MAAM,CAAC,EAAE,CAAC4B,QAAQ,CAACI,GAAG,CAACD,MAAM,EAAEZ,IAAI,CAAC;QAC7C;IACF;IAEA7B,SAAS,uBAAuB;QAC9BS,KAAK,sCAAsC;YACzC,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAWC,OAAO;gBAAK;gBAC/B;oBAAED,MAAM;oBAAcC,OAAO;gBAAM;gBACnC;oBAAED,MAAM;oBAAeC,OAAO;gBAAI;gBAClC;oBAAED,MAAM;oBAAcC,OAAO;gBAAI;gBACjC;oBAAED,MAAM;oBAAeC,OAAO;gBAAI;gBAClC;oBAAED,MAAM;oBAAcC,OAAO;gBAAI;gBACjC;oBAAED,MAAM;oBAAeC,OAAO;gBAAI;gBAClC;oBAAED,MAAM;oBAAYC,OAAO;gBAAK;gBAChC;oBAAED,MAAM;oBAAcC,OAAO;gBAAI;gBACjC;oBAAED,MAAM;oBAAYC,OAAO;gBAAI;gBAC/B;oBAAED,MAAM;oBAAcC,OAAO;gBAAI;gBACjC;oBAAED,MAAM;oBAAWC,OAAO;gBAAM;gBAChC;oBAAED,MAAM;oBAAcC,OAAO;gBAAS;gBACtC;oBAAED,MAAM;oBAAYC,OAAO;gBAAI;gBAC/B;oBAAED,MAAM;oBAAcC,OAAO;gBAAM;gBACnC;oBAAED,MAAM;oBAAeC,OAAO;gBAAI;gBAClC;oBAAED,MAAM;oBAAUC,OAAO;gBAAI;gBAC7B;oBAAED,MAAM;oBAAeC,OAAO;gBAAI;gBAClC;oBAAED,MAAM;oBAAUC,OAAO;gBAAI;gBAC7B;oBAAED,MAAM;oBAAeC,OAAO;gBAAI;gBAClC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,iDAAiD;YACpD,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAeC,OAAO;gBAAI;gBAClC;oBAAED,MAAM;oBAAcC,OAAO;gBAAI;gBACjC;oBAAED,MAAM;oBAAYC,OAAO;gBAAK;gBAChC;oBAAED,MAAM;oBAAcC,OAAO;gBAAI;gBACjC;oBAAED,MAAM;oBAAeC,OAAO;gBAAI;gBAClC;oBAAED,MAAM;oBAAeC,OAAO;gBAAI;gBAClC;oBAAED,MAAM;oBAAeC,OAAO;gBAAI;gBAClC;oBAAED,MAAM;oBAAcC,OAAO;gBAAI;gBACjC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;IACF;IAEAR,SAAS,cAAc;QACrBS,KAAK,gCAAgC;YACnC,MAAMqB,QAAQ,IAAI3B,YAAK,CAAC;YACxB,MAAMG,QAAQwB,MAAMC,SAAS;YAC7BpB,OAAOL,MAAMC,IAAI,EAAEsB,IAAI,CAAC;YACxBlB,OAAOL,MAAME,KAAK,EAAEqB,IAAI,CAAC;QAC3B;QAEApB,KAAK,oDAAoD;YACvD,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAcC,OAAO;gBAAI;gBACjC;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;QAEAC,KAAK,2CAA2C;YAC9C,MAAMC,SAASN,eAAe;YAC9BO,OAAOD,QAAQE,OAAO,CAAC;gBACrB;oBAAEL,MAAM;oBAAcC,OAAO;gBAAI;gBACjC;oBAAED,MAAM;oBAAYC,OAAO;gBAAI;gBAC/B;oBAAED,MAAM;oBAAOC,OAAO;gBAAG;aAC1B;QACH;IACF;AACF"}